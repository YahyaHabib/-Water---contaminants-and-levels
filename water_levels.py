# -*- coding: utf-8 -*-
"""Water_levels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jgcYStjil6XicFaQ2KjBDW589yidJoW0
"""

import pandas as pd
import numpy as np

"""# **Data Cleaning**

# Annual Freshwater Withdrawls
"""

freshwater_withdrawals_df = pd.read_csv('annual-freshwater-withdrawals.csv')

freshwater_withdrawals_df.head(10)

freshwater_withdrawals_df.duplicated().sum()

freshwater_withdrawals_df.isnull().sum()

missing_code_rows = freshwater_withdrawals_df[freshwater_withdrawals_df['Code'].isnull()]

missing_code_rows.sample(20)

# Replace missing values in the 'Code' column with 'N/A'
freshwater_withdrawals_df['Code'].fillna('N/A', inplace=True)

freshwater_withdrawals_df.isnull().sum()

freshwater_withdrawals_df.dtypes

# List unique entities to check for inconsistencies
unique_entities = freshwater_withdrawals_df['Entity'].unique()
sorted(unique_entities)

freshwater_withdrawals_df.to_csv('freshwater_withdrawals_cleaned.csv', index=False)

""" # Premature Deaths"""

premature_deaths_df = pd.read_excel('Premature_deaths_due_to_UNSAFE_WASH.xlsx', sheet_name='Sheet1')

premature_deaths_df.head(10)

premature_deaths_df.isnull().sum()

premature_deaths_df.duplicated().sum()

unique_countries = premature_deaths_df['Country'].unique()
sorted(unique_countries)

premature_deaths_df['Health_Impact'].unique()

premature_deaths_df.to_csv('premature_deaths_cleaned.csv', index=False)

"""# Wastewater Discharges"""

wastewater_data = pd.read_excel('/content/Wastewater Discharges Per Year Per Country.xlsx', sheet_name='Sheet1')
wastewater_data.head(20)

wastewater_data.isnull().sum()

wastewater_data.duplicated().sum()

wastewater_data.to_csv('wastewater_cleaned.csv', index=False)

"""# Water Quality"""

water_quality_df = pd.read_excel('/content/Proportion of bodies of water with good ambient water quality.xlsx')

water_quality_df.head(15)

water_quality_df.replace("..", pd.NA, inplace=True)

water_quality_df.duplicated().sum()

water_quality_df[water_quality_df.duplicated()]

# Remove duplicate records
water_quality_df.drop_duplicates(inplace=True)

water_quality_df.isnull().sum()

# Transform the dataset from wide to long format
water_quality_long_df = pd.melt(water_quality_df, id_vars=['Country Name', 'Country Code', 'Series Name', 'Series Code'],
                                var_name='Year', value_name='Proportion of Good Water Quality')



# Extract year from the 'Year' column (which currently includes the year and a string '[YRYEAR]')
water_quality_long_df['Year'] = water_quality_long_df['Year'].str.extract('(\d+)').astype(int)

water_quality_long_df.head()

# Keeping only rows with non-null values in the 'Proportion of Good Water Quality' column
water_quality_filtered_df = water_quality_long_df.dropna(subset=['Proportion of Good Water Quality'])

water_quality_filtered_df[water_quality_filtered_df['Proportion of Good Water Quality']<=0]

# Filter out rows where 'Proportion of Good Water Quality' is less than or equal to 0
water_quality_df = water_quality_filtered_df[~(water_quality_filtered_df['Proportion of Good Water Quality'] <= 0)]

water_quality_filtered_df.to_csv('water_quality_cleaned.csv', index=False)

"""# Death Rate Per 100k"""

death_rate_data = pd.read_csv('/content/death-rates-unsafe-water.csv')

# Display the first few rows of the dataset and its info to understand its structure
death_rate_data.info()

death_rate_data.head(5)

death_rate_data.isnull().sum()

death_rate_data.duplicated().sum()

# Display rows with missing 'Code' values to investigate possible reasons
missing_code_rows = death_rate_data[death_rate_data['Code'].isnull()]

# Show unique entities with missing codes to understand if there's a pattern
unique_entities_missing_code = missing_code_rows['Entity'].unique()

missing_code_rows.head()

unique_entities_missing_code

# Replace missing 'Code' values with 'N/A'
death_rate_data['Code'].fillna('N/A', inplace=True)
# verify if they are still missing
death_rate_data[death_rate_data['Code'].isnull()]

# Update the column name to be more descriptive
death_rate_data.columns = ['Entity', 'Code', 'Year', 'Unsafe Water Death Rate per 100k']

# Verify the updated column name
death_rate_data.head()

# Check year range
year_range = death_rate_data['Year'].min(), death_rate_data['Year'].max()
year_range

death_rate_range = death_rate_data['Unsafe Water Death Rate per 100k'].min(), death_rate_data['Unsafe Water Death Rate per 100k'].max()
death_rate_anomalies = death_rate_data[death_rate_data['Unsafe Water Death Rate per 100k'] < 0]

death_rate_anomalies, death_rate_range,

unique_countries = death_rate_data['Entity'].unique()
sorted(unique_countries)

death_rate_data.to_csv('death_rate_cleaned.csv', index=False)

"""# **Data Integration**

"""

wastewater_cleaned_df = pd.read_csv('/content/wastewater_cleaned.csv')
premature_deaths_cleaned_df = pd.read_csv('/content/premature_deaths_cleaned.csv')
death_rate_cleaned_df = pd.read_csv('/content/death_rate_cleaned.csv')
freshwater_withdrawals_cleaned_df = pd.read_csv('/content/freshwater_withdrawals_cleaned.csv')

# Merge fresh water withdrwals with death rate
merged_df = pd.merge(death_rate_cleaned_df, freshwater_withdrawals_cleaned_df, on=['Entity', 'Year'], how='inner')

merged_df.head()

merged_df.shape

merged_df.isnull().sum()

# Dropping the 'Code_x' and 'Code_y' columns from the merged dataset
merged_df = merged_df.drop(columns=['Code_x', 'Code_y'])

merged_df.head(5)

merged_df.info()

# Rename the 'Country' column to 'Entity' to match the merged dataset
premature_deaths_cleaned_df = premature_deaths_cleaned_df.rename(columns={'Country': 'Entity'})

# Merge the new dataset with the existing merged dataset
merged_df_2 = pd.merge(merged_df, premature_deaths_cleaned_df, on=['Entity', 'Year'], how='inner')

# Display the first few rows of the final merged dataset to verify
merged_df_2.head(20)

merged_df_2.shape

merged_df_2.info()

# Rename the 'Country' column to 'Entity' to match the existing merged dataset
wastewater_cleaned_df = wastewater_cleaned_df.rename(columns={'Country': 'Entity'})

# Merge the new dataset with the existing merged dataset
final_merged = pd.merge(merged_df_2, wastewater_cleaned_df, on=['Entity', 'Year'], how='inner')

final_merged.head(3)

final_merged['Year'].value_counts()

# Check countries with consecutive data from 2015 to 2019
years_required = set(range(2015, 2020))  # Create a set of years from 2015 to 2019

# Filter groups by those that have all the required years
countries_with_consecutive_data = final_merged.groupby('Entity').filter(lambda x: set(x['Year']) >= years_required)['Entity'].unique()

# Display the countries with consecutive data from 2015 to 2019
countries_with_consecutive_data

# Find the years available for each country
years_per_country = final_merged.groupby('Entity')['Year'].apply(list)

years_per_country

# Subset the dataset to include only the specified countries
countries_to_include = [
    'Australia',
    'Belarus',
    'Belgium',
    'Bulgaria',
    'Croatia',
    'Czechia',
    'Denmark',
    'Estonia',
    'Costa Rica'
]

subset_df = final_merged[final_merged['Entity'].isin(countries_to_include)]

subset_df.head()

# Further filter the subset to keep only the years from 2015 to 2019
subset_df_2015_2019 = subset_df[(subset_df['Year'] >= 2015) & (subset_df['Year'] <= 2019)]

subset_df_2015_2019.tail(5)

# Costa Rica's data for 2016 exists and will be used for 2015
# Find Costa Rica's 2016 data
costa_rica_2016_data = subset_df_2015_2019[(subset_df_2015_2019['Entity'] == 'Costa Rica') & (subset_df_2015_2019['Year'] == 2016)]

# Copy 2016 data to create a 2015 entry
nocf_2015_data = costa_rica_2016_data.copy()
nocf_2015_data['Year'] = 2015

# Append the NOCF data to the dataset
subset_df_2015_2019 = pd.concat([subset_df_2015_2019, nocf_2015_data], ignore_index=True)

# Sort the dataset if necessary
subset_df_2015_2019 = subset_df_2015_2019.sort_values(by=['Entity', 'Year'])

subset_df_2015_2019.head(5)

# Filter for Czechia and the specific column
czechia_sea_discharges = subset_df_2015_2019[(subset_df_2015_2019['Entity'] == 'Czechia')]['Total discharges to the sea(million m3)']

# Calculate the mean, excluding zeros
mean_czechia_sea_discharges = czechia_sea_discharges[czechia_sea_discharges != 0].mean()

# Replace the zero value for Czechia in 2015 with the mean
subset_df_2015_2019.loc[(subset_df_2015_2019['Entity'] == 'Czechia') & (subset_df_2015_2019['Year'] == 2015) & (subset_df_2015_2019['Total discharges to the sea(million m3)'] == 0), 'Total discharges to the sea(million m3)'] = mean_czechia_sea_discharges

# Rename the dataframe to a more suitable name representing the whole dataset
water_related_dataset = subset_df_2015_2019.copy()

# Remove the "Risk" column from the water_related_dataset
water_related_dataset = water_related_dataset.drop(columns=['Risk'])


water_related_dataset.head()

# Export the dataframe to a CSV file
water_related_dataset.to_csv('integrated_water_related_data.csv', index=False)